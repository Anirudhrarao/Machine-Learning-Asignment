{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f19aa17-bb7e-46a6-b05f-13927539552b",
   "metadata": {},
   "source": [
    "#### Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e886a42c-ccd1-4ad2-8fb7-1fb9653c0b1b",
   "metadata": {},
   "source": [
    "KNN is also known as k-nearest neighbour algorithm.\n",
    "it is a supervised machine learining algorithm.\n",
    "used for classification and regression.\n",
    "k refers to number of nearest neighbour.\n",
    "To make a prediction for a new data point, the algorithm calculates the distance between that data point and all other data points in the dataset. The K nearest data points are then used to make the prediction, based on the majority class (for classification) or average value (for regression) of the K neighbors.\n",
    "The KNN algorithm is non-parametric, meaning it does not make any assumptions about the underlying distribution of the data. It is also instance-based, meaning it does not learn a model from the training data but rather stores the entire dataset and uses it to make predictions for new data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4759b-3780-48f8-bcbf-3f8a799e92a5",
   "metadata": {},
   "source": [
    "#### Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1560614-605e-4864-914b-9db3f5696f39",
   "metadata": {},
   "source": [
    " there is no fixed rule to choose the value of K in KNN, and it depends on the characteristics of the dataset and the problem at hand. Cross-validation and domain knowledge are two common approaches to select the optimal value of K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c30d21-06ae-47b1-b2c6-6677c7a04b6b",
   "metadata": {},
   "source": [
    "#### Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6690fb-4fce-4859-a264-6443019d247a",
   "metadata": {},
   "source": [
    "In KNN classification, the goal is to predict the class membership of a new data point based on its nearest neighbors. The output of the KNN classifier is a class label, which is a categorical variable. For example, in a binary classification problem, the output can be either 0 or 1, representing the two possible classes.\n",
    "\n",
    "In KNN regression, the goal is to predict a continuous variable (i.e., a numeric value) for a new data point based on its nearest neighbors. The output of the KNN regressor is a numeric value, which can be any real number. For example, in a regression problem that predicts the price of a house based on its features, the output can be any positive real number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc7980-b97c-4add-8294-98bfaab75a09",
   "metadata": {},
   "source": [
    "#### Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe493c-57c3-4fe8-938b-e8f3307f496a",
   "metadata": {},
   "source": [
    "Classification Metrics:\n",
    "\n",
    "    Accuracy: The proportion of correctly classified instances to the total number of instances.\n",
    "    Precision: The proportion of correctly classified positive instances to the total number of predicted positive instances.\n",
    "    Recall: The proportion of correctly classified positive instances to the total number of actual positive instances.\n",
    "    F1-score: The harmonic mean of precision and recall.\n",
    "Regression Metrics:\n",
    "\n",
    "    Mean Absolute Error (MAE): The average absolute difference between the predicted and actual values.\n",
    "    Mean Squared Error (MSE): The average squared difference between the predicted and actual values.\n",
    "    Root Mean Squared Error (RMSE): The square root of the MSE.\n",
    "    R-squared (R2): The proportion of variance in the target variable that is explained by the model.\n",
    "    To evaluate the performance of the KNN algorithm, you can split the dataset into training and testing sets, fit the KNN model on the training set, and then make predictions on the testing set. You can then use the above-mentioned evaluation metrics to measure the performance of the KNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd15284d-2803-4514-9075-e35c5d874233",
   "metadata": {},
   "source": [
    "#### Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b28f0-7446-4192-b90e-283ecc67c4a9",
   "metadata": {},
   "source": [
    "the curse of dimensionality is a problem in KNN where the performance of the algorithm degrades as the number of dimensions in the dataset increases. This can lead to sparsity and increased computational cost, and various approaches such as dimensionality reduction and feature selection can be used to address this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b878bca-19f4-4a88-ab86-505d595fea0c",
   "metadata": {},
   "source": [
    "#### Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9820681-e69a-4cc0-a742-d0da237fe7ac",
   "metadata": {},
   "source": [
    "Deletion: One approach to handling missing values is to simply delete any data points with missing values. However, this approach can lead to a loss of data, especially if the percentage of missing values is high.\n",
    "\n",
    "Imputation: Imputation is the process of filling in missing values with estimated values. There are several imputation techniques that can be used, including mean imputation, median imputation, mode imputation, and k-NN imputation.\n",
    "\n",
    "Distance-based imputation: Distance-based imputation is a technique that is specific to the KNN algorithm. In this approach, the distance between the nearest neighbors is used to impute missing values. For example, if a feature value is missing for a data point, the missing value can be replaced with the weighted average of the feature values of the nearest neighbors, with weights based on their distance from the data point.\n",
    "\n",
    "Algorithm-specific techniques: Some algorithms may have specific techniques for handling missing values. For example, the KNNImputer class in scikit-learn is an implementation of the KNN algorithm specifically designed for imputing missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a3662-3131-4593-98dc-95a4798c3c69",
   "metadata": {},
   "source": [
    "#### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f7544-7aba-422d-9ec2-aaaf1042b61d",
   "metadata": {},
   "source": [
    "The KNN algorithm can be used for both classification and regression problems, and the performance of the KNN classifier and regressor can be evaluated using different metrics. The choice of which one to use depends on the specific problem and the characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef102ca5-100e-48cf-8b07-4dc3cede1679",
   "metadata": {},
   "source": [
    "#### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e7ee9-aca3-44a4-aae9-95bc2d1b453d",
   "metadata": {},
   "source": [
    "Strengths of the KNN algorithm:\n",
    "\n",
    "    Non-parametric: The KNN algorithm is non-parametric, which means that it does not make any assumptions about the underlying distribution of the data. This makes it more flexible than some other algorithms that assume specific distributions.\n",
    "\n",
    "    Intuitive: The KNN algorithm is easy to understand and implement, making it a good choice for beginners or for problems where interpretability is important.\n",
    "\n",
    "    Good performance on small datasets: The KNN algorithm works well on small datasets, especially when the number of features is relatively small.\n",
    "\n",
    "Weaknesses of the KNN algorithm:\n",
    "\n",
    "    Computationally expensive: The KNN algorithm can be computationally expensive, especially for large datasets, since it requires computing distances between all pairs of data points.\n",
    "\n",
    "    Sensitive to the choice of k: The KNN algorithm is sensitive to the choice of k, which can significantly impact the performance of the algorithm.\n",
    "\n",
    "    Prone to overfitting: The KNN algorithm can be prone to overfitting, especially when the number of features is large or when there are noisy or irrelevant features.\n",
    "\n",
    "    Curse of dimensionality: The KNN algorithm can suffer from the curse of dimensionality in high-dimensional spaces, which can cause the algorithm to perform poorly or require a large number of nearest neighbors to make accurate predictions.\n",
    "\n",
    "To address the weaknesses of the KNN algorithm, several techniques can be used:\n",
    "\n",
    "    Use dimensionality reduction techniques, such as PCA or t-SNE, to reduce the number of dimensions in the dataset.\n",
    "\n",
    "    Use feature selection techniques to select the most informative features that can improve the performance of the algorithm.\n",
    "\n",
    "    Use cross-validation techniques to tune the value of k and prevent overfitting.\n",
    "\n",
    "    Use distance metrics that are more appropriate for the specific problem, such as the cosine similarity metric for text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9e2da-a37c-4105-88b5-9375c99577ec",
   "metadata": {},
   "source": [
    "#### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248ffd9-a026-4a2e-a349-3bef26001904",
   "metadata": {},
   "source": [
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN algorithm, where Euclidean distance is more appropriate for continuous variables and sensitive to the outliers, and Manhattan distance is more appropriate for both continuous and categorical variables and robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a8b1f-2bcb-4079-b713-dc10c3c52bca",
   "metadata": {},
   "source": [
    "#### Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de12d288-f968-4525-b0e8-a595fcdc6c72",
   "metadata": {},
   "source": [
    "Feature scaling is an important preprocessing step in KNN algorithm that can significantly impact the performance of the algorithm. The role of feature scaling is to transform the features of the dataset to have the same scale and reduce the impact of the features with larger magnitude on the distance calculation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
