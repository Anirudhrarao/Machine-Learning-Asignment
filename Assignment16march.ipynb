{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "574cd281",
   "metadata": {},
   "source": [
    "#### Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc2c53",
   "metadata": {},
   "source": [
    "In machine learning model overfitting and underfitting two problem are there ouccurs when developing a model.\n",
    "In overfitting model perform well in training but poor performance on test data.\n",
    "In underfiitig model performance and and test performance both are low.\n",
    "\n",
    "overfitting and underfitting are common problems in machine learning. Overfitting occurs when a model is too complex, and underfitting occurs when a model is too simple. Both problems can be mitigated by adjusting the model's complexity, regularization, early stopping, cross-validation, and increasing the training data size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47df12",
   "metadata": {},
   "source": [
    "#### Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9155ddc7",
   "metadata": {},
   "source": [
    "Overfitting is a common problem in machine learning where a model is too complex and fits training data to closely and results into poor performance on new or unseen data. To reduce this problem we have several technique.\n",
    "\n",
    "1. Regularizaion\n",
    "\n",
    "    Add Penalties to loss function to reduce the model complexitiy.\n",
    "2. Dropout \n",
    "    \n",
    "    Randomly dropout the portion of neuron in the layer durig training.\n",
    "3. Early stopping\n",
    "\n",
    "    Monitor the model performance and stop if it is started overfitting.\n",
    "4. Data augmentation\n",
    "\n",
    "    Increase the data size for training.\n",
    "5. Cross-validations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f5eb5",
   "metadata": {},
   "source": [
    "#### Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb38131",
   "metadata": {},
   "source": [
    "Underfitting is a common problem in machine learning where a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and new data. \n",
    "\n",
    "List of scenarios:\n",
    "\n",
    "1. Insufficient data\n",
    "2. Feature selection\n",
    "3. High regularizations\n",
    "4. Isufficient training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89aa3b",
   "metadata": {},
   "source": [
    "#### Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001702af",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between bias and variance and their impact on model performance.\n",
    "\n",
    "\n",
    "Bias refers to the systematic error in a model's predictions that arises due to assumptions or simplifications made during the model's development. Bias can result in underfitting, \n",
    "\n",
    "Variance refers to the amount that the model's predictions vary for different training datasets. Variance can result in overfitting,\n",
    "\n",
    "Reducing bias often increases variance, and reducing variance often increases bias. The goal is to find the optimal balance between bias and variance to achieve good generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220eb2fb",
   "metadata": {},
   "source": [
    "#### Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8cdc7",
   "metadata": {},
   "source": [
    "Train/Validation/Test Split: A common method for detecting overfitting and underfitting is to split the dataset into three parts: a training set, a validation set, and a test set. The model is trained on the training set, and its performance is evaluated on the validation set. If the model performs well on the training set but poorly on the validation set, it may be overfitting. If the model performs poorly on both the training and validation sets, it may be underfitting.\n",
    "\n",
    "Learning Curves: Learning curves show the relationship between the model's performance and the number of training examples. If the training and validation curves converge at a high performance, the model is not overfitting. However, if the training curve continues to improve while the validation curve plateaus, the model may be overfitting. If both the training and validation curves plateau at a low performance, the model may be underfitting.\n",
    "\n",
    "Cross-Validation: Cross-validation is a technique for estimating the performance of a model by partitioning the dataset into multiple folds, training the model on each fold, and evaluating its performance on the remaining folds. If the model's performance varies greatly between folds, it may be overfitting. If the model's performance is consistently poor across all folds, it may be underfitting.\n",
    "\n",
    "Regularization: Regularization is a technique for reducing overfitting by adding a penalty term to the loss function during training. If the regularization parameter is set too high, the model may underfit. If it is set too low, the model may overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d421d2",
   "metadata": {},
   "source": [
    "#### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02022d39",
   "metadata": {},
   "source": [
    "Bias refers to the systematic error in a model's predictions that arises due to assumptions or simplifications made during the model's development. Bias can result in underfitting,\n",
    "\n",
    "Variance refers to the amount that the model's predictions vary for different training datasets. Variance can result in overfitting,\n",
    "\n",
    "Examples of high bias models include linear regression with few features or a simple decision tree, where the model is too simple to capture the complexity of the data. Examples of high variance models include decision trees with many levels or neural networks with many hidden layers, where the model is too complex and fits the training data too closely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e4c3a",
   "metadata": {},
   "source": [
    "#### Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26554b0",
   "metadata": {},
   "source": [
    "Regularization: Regularization is a technique for reducing overfitting by adding a penalty term to the loss function during training. If the regularization parameter is set too high, the model may underfit. If it is set too low, the model may overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57076dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
