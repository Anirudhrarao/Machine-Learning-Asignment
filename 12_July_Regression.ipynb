{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eca179d-f38b-48eb-a27e-50f5f8da94da",
   "metadata": {},
   "source": [
    "<h3 style='color:blue'>Q1. Theory and Concepts</h3> \n",
    "\n",
    "1. Explain the concept of batch normalization in the context of Artificial Neural Networks\n",
    "2. Describe the benefits of using batch normalization during training\n",
    "3. Discuss the working principle of batch normalization, including the normalization step and the learnable\n",
    "parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f832c30-05ea-4749-908b-ce751b7f361a",
   "metadata": {},
   "source": [
    "#### 1. Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a799b3-ab0d-445d-ba21-7ff5d837ba3a",
   "metadata": {},
   "source": [
    "Batch normalization is a technique used in artificial neural networks to improve the training process. It works by normalizing the input data within each mini-batch of training examples.\n",
    "which aids in faster convergence and better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b86a891-0e1d-460e-a54d-9787236449cc",
   "metadata": {},
   "source": [
    "#### 2.Answer\n",
    "- It helps alleviate the vanishing gradient problem, making it easier for the network to learn.\n",
    "- It reduces the sensitivity of the network to the initialization of weights, allowing for faster convergence.\n",
    "- It acts as a regularizer, reducing overfitting and improving generalization.\n",
    "- It makes the network more robust to changes in the input distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c3be9-7a37-47f5-81e6-b18166cfb684",
   "metadata": {},
   "source": [
    "#### 3.Answer\n",
    "Batch normalization works by normalizing the input data within mini-batches during training. This involves calculating the mean and standard deviation of the data, normalizing it to have a mean of zero and a standard deviation of one, and then scaling and shifting it using learnable parameters called gamma and beta. This helps the network converge faster, be less sensitive to weight initialization, and act as a regularizer to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23299e58-f753-41af-b558-b453133af7a7",
   "metadata": {},
   "source": [
    "<h3 style='color:blue'>Q2. Implementation</h3>\n",
    "\n",
    "1. Choose a dataset of your choice (e.g., MNIST, CIFAR-0) and preprocess itr\n",
    "2. Implement a simple feedforward neural network using any deep learning framework/library (e.g.,\n",
    "Tensorflow, pyTorch)r\n",
    "3. Train the neural network on the chosen dataset without using batch normalizationr\n",
    "4. Implement batch normalization layers in the neural network and train the model againr\n",
    "5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and\n",
    "without batch normalizationr\n",
    "6. Discuss the impact of batch normalization on the training process and the performance of the neural\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "556dfc8a-4b4d-4ca1-a49e-a271462b83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92e4679-0428-4bd7-a4e5-ff796da7be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c29596fb-e806-49ba-ad7c-8ac46e2d30d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 11s 5ms/step - loss: 0.2520 - accuracy: 0.9280 - val_loss: 0.1184 - val_accuracy: 0.9640\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1243 - accuracy: 0.9635 - val_loss: 0.0969 - val_accuracy: 0.9703\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0930 - accuracy: 0.9715 - val_loss: 0.0785 - val_accuracy: 0.9755\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0730 - accuracy: 0.9779 - val_loss: 0.0780 - val_accuracy: 0.9766\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0616 - accuracy: 0.9809 - val_loss: 0.0690 - val_accuracy: 0.9788\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0529 - accuracy: 0.9832 - val_loss: 0.0677 - val_accuracy: 0.9787\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0457 - accuracy: 0.9854 - val_loss: 0.0740 - val_accuracy: 0.9787\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0404 - accuracy: 0.9871 - val_loss: 0.0716 - val_accuracy: 0.9778\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0360 - accuracy: 0.9888 - val_loss: 0.0755 - val_accuracy: 0.9767\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0352 - accuracy: 0.9884 - val_loss: 0.0784 - val_accuracy: 0.9771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1abee84d610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Flatten(input_shape=(28,28)))\n",
    "model.add(layers.Dense(128))\n",
    "# Adding batch normalization\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf00b33d-09bb-4419-99dd-e3c8bd0799bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2578 - accuracy: 0.9269 - val_loss: 0.1435 - val_accuracy: 0.9561\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1149 - accuracy: 0.9667 - val_loss: 0.0987 - val_accuracy: 0.9701\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0798 - accuracy: 0.9758 - val_loss: 0.0879 - val_accuracy: 0.9742\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0600 - accuracy: 0.9815 - val_loss: 0.0821 - val_accuracy: 0.9751\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0453 - accuracy: 0.9855 - val_loss: 0.0795 - val_accuracy: 0.9763\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0364 - accuracy: 0.9885 - val_loss: 0.0789 - val_accuracy: 0.9773\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0298 - accuracy: 0.9909 - val_loss: 0.0857 - val_accuracy: 0.9762\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0229 - accuracy: 0.9929 - val_loss: 0.0776 - val_accuracy: 0.9796\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0188 - accuracy: 0.9945 - val_loss: 0.0888 - val_accuracy: 0.9769\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0162 - accuracy: 0.9948 - val_loss: 0.0786 - val_accuracy: 0.9792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1abee621190>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without batch normalization\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Flatten(input_shape=(28,28)))\n",
    "model.add(layers.Dense(128))\n",
    "\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d46908-4b79-494b-acd7-dbd9382b2d3e",
   "metadata": {},
   "source": [
    "##### As you can see with batch normalization The model achieves high accuracy on the validation set, reaching around 97.7% after 10 epochs. and without normalization the model achieves a lower validation accuracy of around 97.9% after 10 epochs compared to the model with batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b84f11-9b3e-44fe-b353-098d11213204",
   "metadata": {},
   "source": [
    "<h2 style='color:blue'>Q3. Experimentation and Analysis</h2>\n",
    "\n",
    "1. Experiment with different batch sizes and observe the effect on the training dynamics and model\n",
    "performancer\n",
    "2. Discuss the advantages and potential limitations of batch normalization in improving the training of\n",
    "neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f40fc923-22bf-46de-bc1d-ae13cc17ceb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch size: 16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m model_no_bn\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, validation_data\u001b[38;5;241m=\u001b[39m(x_test, y_test), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Train the model with batch normalization\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mmodel_bn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Evaluate and compare the models\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel without batch normalization:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1676\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1674\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m   1675\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 1676\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   1677\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m             epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m             _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m         ):\n\u001b[0;32m   1684\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1375\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1376\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1377\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[0;32m   1380\u001b[0m )\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:647\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    646\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    648\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    649\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:774\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \n\u001b[0;32m    767\u001b[0m \u001b[38;5;124;03mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;124;03m  The value of the variable.\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 774\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;66;03m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array_ops\u001b[38;5;241m.\u001b[39midentity(value)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:753\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    751\u001b[0m       result \u001b[38;5;241m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 753\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    756\u001b[0m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    757\u001b[0m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m   tape\u001b[38;5;241m.\u001b[39mrecord_operation(\n\u001b[0;32m    759\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadVariableOp\u001b[39m\u001b[38;5;124m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle],\n\u001b[0;32m    760\u001b[0m       backward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    761\u001b[0m       forward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:743\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_copy \u001b[38;5;129;01mand\u001b[39;00m forward_compat\u001b[38;5;241m.\u001b[39mforward_compatible(\u001b[38;5;241m2022\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m    742\u001b[0m   gen_resource_variable_ops\u001b[38;5;241m.\u001b[39mdisable_copy_on_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m--> 743\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    745\u001b[0m _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, result)\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:580\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    579\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 580\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReadVariableOp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    583\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "def create_model(use_batch_norm=False):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    model.add(layers.Dense(128))\n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())  # Add batch normalization layer\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Experiment with different batch sizes\n",
    "batch_sizes = [16, 32, 64, 128, 256]\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Training with batch size: {batch_size}\")\n",
    "    \n",
    "    # Create and compile the models\n",
    "    model_no_bn = create_model(use_batch_norm=False)\n",
    "    model_bn = create_model(use_batch_norm=True)\n",
    "    model_no_bn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model_bn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model without batch normalization\n",
    "    model_no_bn.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Train the model with batch normalization\n",
    "    model_bn.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Evaluate and compare the models\n",
    "    print(\"Model without batch normalization:\")\n",
    "    model_no_bn.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"Model with batch normalization:\")\n",
    "    model_bn.evaluate(x_test, y_test, verbose=0)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbe7971-507c-4d9d-a48c-ca8b64afede7",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "- Faster convergence and accelerated training.\n",
    "- Improved model generalization and reduced overfitting.\n",
    "- Reduced sensitivity to weight initialization.\n",
    "- Enhanced stability and robustness during training.\n",
    "- Reduced dependence on manual hyperparameter tuning.ing.  as RNNs.\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca32c3-3451-4340-a029-21177b9e8d40",
   "metadata": {},
   "source": [
    "#### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369f230-7404-425f-83b7-2cbd28c791e9",
   "metadata": {},
   "source": [
    "- Increased memory usage.\n",
    "- Sensitivity to batch size choice.\n",
    "- Dependency on mini-batch statistics during inference.\n",
    "- Added computational overhead.\n",
    "- Incompatibility with certain architectures, such as RNNs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
