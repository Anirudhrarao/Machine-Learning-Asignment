{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bbe0fb-5c29-44c9-82cd-4fb2b4aaa1de",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4 style='color:#98D8AA'>Q1. What is anomaly detection and what is its purpose?</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2f76a2-9162-44c7-8e0d-7d07667d210a",
   "metadata": {},
   "source": [
    "<div style='color:#2A2F4F;border:6px double black;border-radius:10px;padding:10px;background:#BFCCB5;'>\n",
    "    Anomaly detection is the process of identifying patterns or data points that deviate significantly from the normal behavior of a system or dataset. The purpose of anomaly detection is to detect unusual or suspicious behavior that may indicate a potential threat, fraud, or errors in the system. Anomaly detection is widely used in various fields, including cybersecurity, finance, healthcare, and manufacturing, to detect and prevent anomalies that could cause harm or damage to the system or the organization.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dab6bc-b355-4431-9166-ff7dcea29574",
   "metadata": {},
   "source": [
    "<h4 style='color:#98D8AA'>Q2. What are the key challenges in anomaly detection?</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7476a7ba-6fe1-41c4-80fb-fc618cae0451",
   "metadata": {},
   "source": [
    "<div style='color:#2A2F4F;border:6px double black;border-radius:10px;padding:10px;background:#BFCCB5;'>\n",
    "Lack of labeled data: In many cases, anomalies are rare and difficult to label, making it challenging to train accurate models.\n",
    "\n",
    "Imbalanced data: Anomalies often represent a small portion of the overall data, which can lead to imbalanced datasets that are difficult to model.\n",
    "\n",
    "Dynamic environments: Real-world systems are often dynamic and can change rapidly, making it challenging to adapt models to changing conditions.\n",
    "\n",
    "Feature engineering: Identifying the right features to use in anomaly detection models can be difficult, especially in complex systems with many variables.\n",
    "\n",
    "False positives: Anomaly detection models may generate false alarms, which can be costly and time-consuming to investigate.\n",
    "\n",
    "Interpretability: Understanding why a particular data point or pattern was flagged as an anomaly can be challenging, especially for complex models. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42d2d9-1255-4cb6-a0da-77d2b246d632",
   "metadata": {},
   "source": [
    "<h4 style='color:#98D8AA'>Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b3e6c-50dd-4a51-a69e-531716bf9801",
   "metadata": {},
   "source": [
    "<div style='color:#2A2F4F;border:6px double black;border-radius:10px;padding:10px;background:#BFCCB5;'>\n",
    " Unsupervised anomaly detection: In this approach, the algorithm is not provided with labeled data that indicates which instances are anomalous. Instead, it relies on identifying patterns or data points that deviate significantly from the norm in an unsupervised manner. This approach is useful when labeled data is scarce or unavailable.\n",
    "\n",
    "Supervised anomaly detection: In this approach, the algorithm is trained on labeled data that indicates which instances are anomalous. The algorithm learns to differentiate between normal and anomalous instances based on the labeled data. This approach is useful when labeled data is available and the goal is to identify anomalies that are similar to those seen in the training data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc560b-24ac-41e3-a3df-b8bd7ecb53f4",
   "metadata": {},
   "source": [
    "<h4 style='color:#98D8AA'>Q4. What are the main categories of anomaly detection algorithms?</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1552cb-3344-401c-a18e-2051ea54e33f",
   "metadata": {},
   "source": [
    "<div style='color:#2A2F4F;border:6px double black;border-radius:10px;padding:10px;background:#BFCCB5;'>\n",
    " Statistical-based algorithms: These algorithms use statistical methods to identify anomalies based on the assumption that anomalies have different statistical properties than normal data points.\n",
    "\n",
    "Machine learning-based algorithms: These algorithms use machine learning techniques, such as clustering or classification, to identify anomalies. They learn from labeled or unlabeled data and can be either supervised or unsupervised.\n",
    "\n",
    "Deep learning-based algorithms: These algorithms use deep neural networks to identify anomalies in complex datasets. They are particularly useful for identifying anomalies in image, video, and audio data.\n",
    "\n",
    "Rule-based algorithms: These algorithms use a set of predefined rules to identify anomalies. They are often used in systems with specific rules and regulations that must be followed.\n",
    "\n",
    "Hybrid algorithms: These algorithms combine multiple techniques from the above categories to improve anomaly detection performance. For example, a hybrid algorithm may use a statistical-based approach to preprocess the data and a machine learning-based approach to classify anomalies.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d048a-640e-4d15-9b02-586f3af5d9df",
   "metadata": {},
   "source": [
    "<h4 style='color:#98D8AA'>Q5. What are the main assumptions made by distance-based anomaly detection methods?</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071368bc-0124-4c23-bc2f-c403f3a3ec64",
   "metadata": {},
   "source": [
    "<div style='color:#2A2F4F;border:6px double black;border-radius:10px;padding:10px;background:#BFCCB5;'>\n",
    " Normal data points are dense and tightly clustered in the feature space, while anomalies are far away from the normal data points and less dense.\n",
    "\n",
    "The distance between a data point and its k-nearest neighbors can be used to determine whether the data point is an anomaly or not. Anomalies are those data points that have a large distance to their k-nearest neighbors.\n",
    "\n",
    "The distribution of the normal data points in the feature space is assumed to be smooth and continuous. Anomalies are those data points that are far away from this smooth distribution.\n",
    "\n",
    "Distance-based methods assume that the feature space is Euclidean or can be transformed to be Euclidean. In other words, the distance between any two points can be computed using a distance metric such as Euclidean distance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dab24f-40a1-4e49-a1fb-147e5e8b257a",
   "metadata": {},
   "source": [
    "<h4 style='color:#98D8AA'>Q6. How does the LOF algorithm compute anomaly scores?</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c744db-c527-4eec-b147-47de17070c52",
   "metadata": {},
   "source": [
    "<div style='color:#2A2F4F;border:6px double black;border-radius:10px;padding:10px;background:#BFCCB5;'>\n",
    " For each data point, the k-distance is computed, which is the distance to its k-th nearest neighbor.\n",
    "\n",
    "The k-distance is used to define a k-nearest neighbor (k-NN) region around each data point.\n",
    "\n",
    "The reachability distance of each data point is computed as the maximum of the k-distance of the data point and the distance between the data point and its neighbor.\n",
    "\n",
    "The local reachability density (LRD) of each data point is computed as the inverse of the average reachability distance of its k-nearest neighbors.\n",
    "\n",
    "The local outlier factor (LOF) of each data point is computed as the ratio of the LRD of the data point and the LRD of its k-nearest neighbors. LOF values greater than 1 indicate that a data point is an outlier, while values less than 1 indicate that a data point is similar to its neighbors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab116b9e-5a0a-4efd-95a9-9b24a98a0d85",
   "metadata": {},
   "source": [
    "<h4 style='color:#98D8AA'>Q7. What are the key parameters of the Isolation Forest algorithm?</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1416152-5c9f-4402-83f0-bf0767d43c9e",
   "metadata": {},
   "source": [
    "<div style='color:#2A2F4F;border:6px double black;border-radius:10px;padding:10px;background:#BFCCB5;'>\n",
    " The Isolation Forest algorithm has two key parameters:\n",
    "\n",
    "The number of trees (n_estimators): This parameter specifies the number of trees to be used in the ensemble. A larger number of trees increases the chances of detecting anomalies, but also increases the computational time.\n",
    "\n",
    "The maximum depth of the trees (max_depth): This parameter specifies the maximum depth of each tree in the ensemble. A larger maximum depth allows the trees to better fit the data, but also increases the risk of overfitting.\n",
    "\n",
    "The Isolation Forest algorithm also has optional parameters, such as the subsampling size (max_samples) and the contamination parameter, which controls the proportion of anomalies in the dataset. The subsampling size controls the number of data points to be randomly selected at each split of the tree, while the contamination parameter determines the expected proportion of anomalies in the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5a401-b298-49a6-82d1-edfd25cfeb8d",
   "metadata": {},
   "source": [
    "<h4 style='color:#98D8AA'>Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac9e958-3124-4862-91d6-8850de890cd0",
   "metadata": {},
   "source": [
    "<div style='color:#2A2F4F;border:6px double black;border-radius:10px;padding:10px;background:#BFCCB5;'>\n",
    " the anomaly score of a data point with only 2 neighbors of the same class within a radius of 0.5 using KNN with K=10 cannot be determined without additional information about the distances to its other neighbors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b43e7d-f09a-473a-ae38-1179e5c3b16c",
   "metadata": {},
   "source": [
    "<h4 style='color:#98D8AA'>Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0610c0-7c38-46e8-8cb1-0642e371e42b",
   "metadata": {},
   "source": [
    "<div style='color:#2A2F4F;border:6px double black;border-radius:10px;padding:10px;background:#BFCCB5;'>\n",
    " if the average path length of the trees in the forest is 10.0, and the data point has an average path length of 5.0, the anomaly score of the data point would be:\n",
    "\n",
    "Anomaly score = 5.0 / 10.0 = 0.5\n",
    "\n",
    "A lower anomaly score indicates a higher likelihood of the data point being an anomaly.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
